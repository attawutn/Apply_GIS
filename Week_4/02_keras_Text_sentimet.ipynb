{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IMDB movie reviews sentiment classification dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads the IMDB dataset that's included with Keras and creates a dictionary mapping the words in all 50,000 reviews to integers indicating the words' relative frequency of occurrence. Each word is assigned a unique integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the inner lists contain numbers rather than text is that you don't train a neural network with text; you train it with numbers.Specifically, you train it with tensors. In this case, each review is a 1-dimensional tensor (think of a 1-dimensional array) containing integers identifying the words contained in the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each word is assigned a unique integer.\n",
    "#The most common word is assigned the number 1, the second most common word is assigned the number 2, and so on.\n",
    "\n",
    "from keras.datasets import imdb\n",
    "top_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable named x_train is a list of 25,000 lists, each of which represents one movie review. (x_test is also a list of 25,000 lists representing 25,000 reviews. x_train will be used for training, while x_test will be used for testing.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the dictionary looks like?\n",
    "#Dictionary mapping words to integers\n",
    "imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse-encode\n",
    "word_dict = imdb.get_word_index()\n",
    "word_dict = { key:(value + 3) for key, value in word_dict.items() }\n",
    "word_dict[''] = 0  # Padding\n",
    "word_dict['>'] = 1 # Start\n",
    "word_dict['?'] = 2 # Unknown word\n",
    "reverse_word_dict = { value:key for key, value in word_dict.items() }\n",
    "print(' '.join(reverse_word_dict[id] for id in x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, \">\" marks the beginning of the review, while \"?\" marks words that aren't among the most common 10,000 words in the dataset. These \"unknown\" words are represented by 2s in the list of integers representing a review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you train a neural network with collection of tensors, each tensor needs to be the same length. \n",
    "#Keras includes a function that takes a list of lists as input and converts the inner lists to a specified length by truncating them if necessary or padding them with 0s. \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "max_review_length = 500                    # x_train and x_test to a length of 500 integers\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks to perform a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a neural network with Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))      #The embedding layer essentially maps many-dimensional arrays containing integer word indexes into floating-point arrays containing fewer dimensions.\n",
    "model.add(Flatten())                                                                          #Reshapes the output for input to the next layer\n",
    "#the hidden layers\n",
    "model.add(Dense(16, activation='relu'))                                                       #Fully connected layers with 16 neurons\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))                                                     #Fully connected layers with 1 neuron to predict one output,  a sentiment score from 0.0 to 1.0\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])              #Compiles the model by specifying paramrters like: loss-function, metrics and the optimizer\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# epoch = 5 tells to keras to make 5 forward and backward passes through the model\n",
    "# batch_size=128 tells to keras to use 128 training samples at a time to train the network. \n",
    "# NOTE : Smaller batch sizes sometimes increase accuracy. \n",
    "\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One indication that a model is overfitting is a growing discrepancy between the training accuracy and the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the changes in training and validation accuracy as training progress\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot indicates that the training accuracy is bigger than the vaalidation accuracy so that there is overfitting for that reason we have to perform a regularization over the model, in order the model can be more simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check for overfitting is to compare training loss to validation loss as training proceeds. Where for a given epoch, training loss, much lower than validation loss, can be evidence of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "loss = hist.history['loss']\n",
    "val = hist.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, '-', label='Training loss')\n",
    "plt.plot(epochs, val, ':', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine how accurately the model is able to quantify the sentiment expressed in text based on the test data in x_test (reviews) and y_test (0s and 1s, or \"labels,\" indicating which reviews are positive and which are negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " input text of your own into the model and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def analyze(text):\n",
    "    # Prepare the input by removing punctuation characters, converting\n",
    "    # characters to lower case, and removing words containing numbers\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    text = text.lower().split(' ')\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "\n",
    "    # Generate an input tensor\n",
    "    input = [1]\n",
    "    for word in text:\n",
    "        if word in word_dict and word_dict[word] < top_words:\n",
    "            input.append(word_dict[word])\n",
    "        else:\n",
    "            input.append(2)\n",
    "    padded_input = sequence.pad_sequences([input], maxlen=max_review_length)\n",
    "\n",
    "    # Invoke the model and return the result\n",
    "    result = model.predict(np.array([padded_input][0]))[0][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function accepts a string as input and returns a number from 0.0 to 1.0 quantifying the sentiment expressed in that string. The higher the number, the more positive the sentiment. The function cleans the input string, converts it into a list of integers referencing words in the dictionary created by the load_data function, and finally calls the model's predict function to score the text for sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaple 1\n",
    "analyze('Easily the most stellar experience I have ever had.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exaxmple 2\n",
    "analyze('The long lines and poor customer service really turned me off.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
